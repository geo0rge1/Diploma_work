{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84e486a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textacy\n",
    "from spacy import displacy\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f034d0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"ru_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe491ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class text_treatment:\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.clean_sent = []\n",
    "    def clean(self):\n",
    "        #removing paragraph numbers\n",
    "        self.text = re.sub('[0-9]+.\\t','',str(self.text))\n",
    "        #removing new line characters\n",
    "        self.text = re.sub('\\n ','',str(self.text))\n",
    "        self.text = re.sub('\\n',' ',str(self.text))\n",
    "        #removing apostrophes\n",
    "        self.text = re.sub(\"'s\",'',str(self.text))\n",
    "        #removing hyphens\n",
    "        self.text = re.sub(\"-\",' ',str(self.text))\n",
    "        self.text = re.sub(\" â€“ \",' ',str(self.text))\n",
    "        #removing quotation marks\n",
    "        self.text = re.sub('\\\"','',str(self.text))\n",
    "        #removing any reference to outside text\n",
    "        self.text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", str(self.text))\n",
    "        #removing urls\n",
    "        self.text = re.sub(r'http\\S+', \"\", str(self.text))\n",
    "        #removing hashtags\n",
    "        self.text = re.sub('#',\"\", str(self.text))\n",
    "    def sentences(self):\n",
    "    # split sentences and questions\n",
    "        self.text = re.split('(?<!\\..)[.?!]\\s+', self.text)\n",
    "        for sent in self.text:\n",
    "            self.clean_sent.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "775c4256",
   "metadata": {},
   "outputs": [],
   "source": [
    "class text_triplets:\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "        self.verb_patterns = [[{\"POS\":\"VERB\"},{\"POS\":\"ADP\"}],[{\"POS\":\"VERB\"}],[{\"POS\":\"ADV\"},{\"POS\":\"VERB\"}]]\n",
    "        self.noun_patterns = [[{\"POS\":\"NOUN\"}],[{\"POS\":\"PROPN\"}],[{\"POS\":\"PROPN\"},{\"POS\":\"PORPN\"}],[{\"POS\":\"ADJ\"},{\"POS\":\"ADJ\"},{\"POS\":\"NOUN\"}],[{\"POS\":\"ADJ\"},{\"POS\":\"NOUN\"}],[{\"POS\":\"NOUN\"},{\"POS\":\"PROPN\"}],[{\"POS\":\"DET\"},{\"POS\":\"PROPN\"}],[{\"POS\":\"DET\"},{\"POS\":\"NOUN\"}],[{\"POS\":\"DET\"},{\"POS\":\"NOUN\"},{\"POS\":\"PROPN\"}]]\n",
    "        self.edges = []\n",
    "        self.verbp = []\n",
    "        self.edges_2 = []\n",
    "        self.verbp_2 = []\n",
    "        \n",
    "    def contains_root(self, verb_phrase, root):\n",
    "        vp_start = verb_phrase.start\n",
    "        vp_end = verb_phrase.end\n",
    "        if (root.i >= vp_start and root.i <= vp_end):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def find_root_of_sentence(self, doc):\n",
    "        root_token = None\n",
    "        for token in doc:\n",
    "            if (token.dep_ == \"ROOT\"):\n",
    "                root_token = token\n",
    "        return root_token\n",
    "    \n",
    "    def get_verb_phrases(self, doc):\n",
    "        root = self.find_root_of_sentence(doc)\n",
    "        verb_phrases = textacy.extract.matches.token_matches(doc, self.verb_patterns)\n",
    "        new_vps = []\n",
    "        for verb_phrase in verb_phrases:\n",
    "            if (self.contains_root(verb_phrase, root)):\n",
    "                new_vps.append(verb_phrase)\n",
    "        return new_vps\n",
    "    \n",
    "    def longer_verb_phrase(self, verb_phrases):\n",
    "        longest_length = 0\n",
    "        longest_verb_phrase = None\n",
    "        for verb_phrase in verb_phrases:\n",
    "            if len(verb_phrase) > longest_length:\n",
    "                longest_length = len(verb_phrase)\n",
    "                longest_verb_phrase = verb_phrase\n",
    "        return longest_verb_phrase\n",
    "    \n",
    "    def find_noun_phrases(self, doc):\n",
    "        noun_phrases = textacy.extract.matches.token_matches(doc,self.noun_patterns)\n",
    "        new_nph = []\n",
    "        for noun_phrase in noun_phrases:\n",
    "            new_nph.append(noun_phrase)\n",
    "        return new_nph\n",
    "        \n",
    "    def longer_noun_phrase(self, noun_phrases):\n",
    "        longest_length = 0\n",
    "        noun_phrase_temp = None\n",
    "        longest_noun_phrases = []\n",
    "        for noun_phrase in noun_phrases:\n",
    "            if noun_phrase_temp == None:\n",
    "                longest_length = len(noun_phrase)\n",
    "                noun_phrase_temp = noun_phrase\n",
    "            if (str(noun_phrase_temp) in str(noun_phrase)) or (str(noun_phrase) in str(noun_phrase_temp)) and (noun_phrase_temp != None):\n",
    "                if len(noun_phrase) > longest_length:\n",
    "                    longest_lenght = len(noun_phrase)\n",
    "                    noun_phrase_temp = noun_phrase\n",
    "            elif (str(noun_phrase_temp) not in str(noun_phrase)) and (str(noun_phrase) not in str(noun_phrase_temp)) and (noun_phrase_temp != None):\n",
    "                longest_noun_phrases.append(noun_phrase_temp)\n",
    "                noun_phrase_temp = noun_phrase\n",
    "                longest_length = len(noun_phrase)\n",
    "        longest_noun_phrases.append(noun_phrase_temp)\n",
    "        return longest_noun_phrases\n",
    "    \n",
    "    def find_triplet(self, sentence):\n",
    "        doc = nlp(sentence)\n",
    "        verb_phrases = self.get_verb_phrases(doc)\n",
    "        noun_phrases = self.find_noun_phrases(doc)\n",
    "        verb_phrase = None\n",
    "        noun_phrase = None\n",
    "        if (len(verb_phrases) > 1):\n",
    "            verb_phrase = self.longer_verb_phrase(list(verb_phrases))\n",
    "        else:\n",
    "            verb_phrase = verb_phrases[0]\n",
    "        if (len(noun_phrases) > 2):\n",
    "            noun_phrase = self.longer_noun_phrase(noun_phrases)\n",
    "        else:\n",
    "            noun_phrase = noun_phrases\n",
    "        if len(noun_phrase) == 2:\n",
    "            left_noun_phrase = noun_phrase[0]\n",
    "            right_noun_phrase = noun_phrase[1]\n",
    "        else:\n",
    "            words = str(sentence).split()\n",
    "            left_noun_phrase =[]\n",
    "            right_noun_phrase = []\n",
    "            verb_pos = None\n",
    "            i = 0\n",
    "            for j in range(len(words)):\n",
    "                if words[j] in str(verb_phrase) and len(words[j]) > 4:\n",
    "                    verb_pos = i\n",
    "                if words[j] in str(noun_phrase) and verb_pos == None:\n",
    "                    left_noun_phrase.append(noun_phrase[i])\n",
    "                    i += 1\n",
    "                else:\n",
    "                    right_noun_phrase = noun_phrase[i:len(noun_phrase)]   \n",
    "        return (left_noun_phrase, verb_phrase, right_noun_phrase)\n",
    "\n",
    "        \n",
    "    def triplets_array(self):\n",
    "        for sentence in self.sentences:\n",
    "            (a,b,c) = self.find_triplet(sentence)\n",
    "            self.edges.append([a,c])\n",
    "            self.verbp.append(b)\n",
    "        c = 0\n",
    "        for i in self.edges:\n",
    "            if type(i[0]) == list:\n",
    "                if type(i[1]) == list:\n",
    "                    for a in i[0]:\n",
    "                        for b in i[1]:\n",
    "                            self.edges_2.append([a,b])\n",
    "                            self.verbp_2.append(self.verbp[c])\n",
    "                else:\n",
    "                    for a in i[0]:\n",
    "                        self.edges_2.append([a,i[1]])\n",
    "                        self.verbp_2.append(self.verbp[c])\n",
    "            else:\n",
    "                if type(i[1]) == list:\n",
    "                    for b in i[1]:\n",
    "                        self.edges_2.append([i[0],b])\n",
    "                        self.verbp_2.append(self.verbp[c])\n",
    "                else:\n",
    "                    self.edges_2.append([i[0],i[1]])\n",
    "                    self.verbp_2.append(self.verbp[c])\n",
    "            c += 1\n",
    "            \n",
    "    def lemmatize(self):\n",
    "        for i in range(len(self.edges_2)):\n",
    "            for j in range(len(self.edges_2[i])):\n",
    "                self.edges_2[i][j] = self.edges_2[i][j].lemma_\n",
    "            self.verbp_2[i] = self.verbp_2[i].lemma_\n",
    "    \n",
    "    def displacy_show(self):\n",
    "        for sentence in self.sentences:\n",
    "            sentence = nlp(sentence)\n",
    "            displacy.render(sentence, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60b897ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class visualizer:\n",
    "    def __init__(self,edges,edges_labels):\n",
    "        self.edges = edges\n",
    "        self.labels = edges_labels\n",
    "        self.edge_labels = {}\n",
    "    \n",
    "    def edge_labels_create(self):\n",
    "        for i in range(len(self.edges)):\n",
    "            self.edge_labels[tuple(self.edges[i])] = self.labels[i]\n",
    "    \n",
    "    def visualize(self):\n",
    "        edges = self.edges\n",
    "        G = nx.DiGraph()\n",
    "        G.add_edges_from(edges)\n",
    "        pos = nx.spring_layout(G, k=.6)\n",
    "        plt.figure(figsize=(17,15))\n",
    "        nx.draw(\n",
    "            G, pos, edge_color='black', width=1, linewidths=1,\n",
    "            node_size=4300, node_color='pink',node_shape=\"o\", alpha=1,arrows=True,\n",
    "            labels={node: node for node in G.nodes()}\n",
    "        )\n",
    "        nx.draw_networkx_edge_labels(\n",
    "            G, pos,\n",
    "            edge_labels=self.edge_labels,\n",
    "            font_color='red'\n",
    "        )\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6ae589",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
